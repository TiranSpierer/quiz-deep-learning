// Test 003 - מועד א׳ 2 - יולי 2025 (מועד 87)
export default {
  id: "test-002",
  name: "מועד א׳ 2 - יולי 2025",
  description: "20 שאלות מהמבחן הרשמי",
  questions: [
    {
      question: "כיצד נקרא השלב בלמידה שבו האלגוריתם עובר פעם אחת על כל הדוגמאות בקבוצת האימון?",
      options: [
        "Epoch.",
        "Minibatch.",
        "Iteration.",
        "Decay."
      ],
      correct: 0,
      explanation: "Epoch הוא מעבר אחד על כל הדוגמאות בקבוצת האימון."
    },
    {
      question: "בהינתן רשת נוירונים עמוקה לסיווג רב-מחלקתי לעשר קטגוריות (0-9), עם שכבת פלט מסוג softmax, כיצד נכון להעריך את דיוק הסיווג (Accuracy) של המודל?",
      options: [
        "לחשב את ממוצע הערכים בשכבת ה-softmax.",
        "לבדוק האם הערך המקסימלי של ה-softmax גבוה מסף מסוים (למשל 0.5).",
        "להשוות את האינדקס של הערך הגבוה ביותר ב-softmax למחלקה האמיתית, ולחשב את שיעור ההתאמות מכלל הדגימות.",
        "להשוות את האינדקס של הערך הנמוך ביותר ב-softmax למחלקה האמיתית, ולחשב את שיעור ההתאמות מכלל הדגימות."
      ],
      correct: 2,
      explanation: "דיוק נמדד על ידי השוואת הניבוי (argmax של softmax) לתווית האמיתית."
    },
    {
      question: "במהלך אימון רשת נוירונים באמצעות Stochastic Gradient Descent (SGD) לעיתים נעשה שימוש בטכניקת תזמון קצב הלמידה (learning rate scheduling). מהי הסיבה המרכזית לשימוש בתזמון כזה?",
      options: [
        "כדי לשפר את מהירות ההתכנסות על ידי שמירה על קצב למידה גבוה לאורך כל תהליך האימון.",
        "כדי לאפשר קפיצות קטנות יותר במרחב הפרמטרים בשלבים מתקדמים של האימון, ולהתכנס למינימום טוב יותר.",
        "כדי להאיץ את האימון בכך שקצב הלמידה גדל עם כל epoch.",
        "כדי למנוע מה-gradients להפוך לקטנים מדי לאורך זמן ולשמר קצב שינוי קבוע.",
        "תשובות א׳, ב׳, ג׳ ו-ד׳ אינן נכונות."
      ],
      correct: 1,
      explanation: "תזמון קצב הלמידה מאפשר קפיצות קטנות יותר בשלבים מתקדמים להתכנסות טובה יותר."
    },
    {
      question: "כיצד משפיע מומנטום על עדכון הפרמטרים ברשת נוירונים?",
      options: [
        "הוא מאיץ את שינוי הפרמטרים כאשר הגרדיאנטים הולכים וקטנים, כדי להימנע ממצבים בהם הלמידה מאטה באזורים שטוחים של פונקציית המחיר.",
        "הוא מחזק תנועות עקביות בכיוון מסוים ומחליש תנודות חדות בכיוונים משתנים.",
        "הוא מאפס את הגרדיאנט כאשר כיוון הירידה משתנה בין אפוקים (epochs) עוקבים.",
        "הוא מגביר את קצב הלמידה ככל שיותר אפוקים (epochs) נצברים.",
        "תשובות א׳, ב׳, ג׳ ו-ד׳ נכונות.",
        "תשובות א׳, ב׳, ג׳ ו-ד׳ אינן נכונות."
      ],
      correct: 1,
      explanation: "מומנטום מחזק תנועות עקביות ומחליש תנודות, מה שמאיץ התכנסות."
    },
    {
      question: "מהו היתרון המרכזי של שימוש באופטימייזר עם קצב למידה אדפטיבי, כמו Adam או RMSprop?",
      options: [
        "הוא מבטיח התכנסות מהירה יותר מכל אופטימייזר אחר, ללא תלות בנתונים או בארכיטקטורת הרשת.",
        "הוא מתאים את קצב הלמידה באופן דינמי לכל פרמטר, בהתאם להיסטוריית הגרדיאנטים שלו.",
        "הוא שומר על קצב למידה נמוך בתחילת האימון, ומעלה אותו באופן הדרגתי לאורך האפוקים (epochs).",
        "הוא מבצע נרמול של כל גרדיאנט לפרמטר כך שלכולם תהיה השפעה שווה על העדכון.",
        "תשובות א׳, ב׳, ג׳ ו-ד׳ נכונות.",
        "תשובות א׳, ב׳, ג׳ ו-ד׳ אינן נכונות."
      ],
      correct: 1,
      explanation: "Adam ו-RMSprop מתאימים את קצב הלמידה לכל פרמטר בנפרד לפי היסטוריית הגרדיאנטים."
    },
    {
      question: "במהלך עיבוד תמונה באמצעות רשת נוירונים קונבולוציונית (CNN) נעשה שימוש בגרעין קונבולוציה בגודל 3x3 כדי לזהות קצוות בתמונה. איזה מהגרעינים הבאים צפוי לזהות קצוות אופקיים (horizontal) בצורה היעילה ביותר?",
      options: [
        "[[1, 0, -1], [1, 0, -1], [1, 0, -1]].",
        "[[0, 1, 0], [1, 0, -1], [0, -1, 0]].",
        "[[1, 1, 1], [0, 0, 0], [-1, -1, -1]].",
        "[[-1, 0, 1], [0, 0, 0], [1, 0, -1]]."
      ],
      correct: 2,
      explanation: "גרעין עם שורה עליונה חיובית ותחתונה שלילית מזהה קצוות אופקיים."
    },
    {
      question: "באילו תנאים שימוש ב-Transfer Learning בלמידה עמוקה נחשב לאפקטיבי?\n\nבחרו את האפשרות המדויקת ביותר:",
      options: [
        "Transfer Learning אפקטיבי כאשר שכבות סופיות של רשת מאומנת מראש נלמדות מחדש (fine-tuning) כיוון שהן מייצגות תכונות גנריות שמתאימות לרוב המשימות.",
        "Transfer Learning מתאים בעיקר למשימות שבהן יש נתוני אימון בכמות גדולה, כיוון שהוא מחזק את הלמידה של המודל המקורי מבלי לשנות את המבנה שלו.",
        "Transfer Learning נשען על ההנחה ששכבות מוקדמות לומדות ייצוגים כלליים הניתנים למיחזור, בעוד שכבות מאוחרות מותאמות למשימה הספציפית ולכן מחליפים או מעדכנים אותן.",
        "Transfer Learning יעיל רק כאשר הארכיטקטורה של המודל המקורי והמודל החדש זהה לחלוטין, כולל מספר שכבות ופרמטרים."
      ],
      correct: 2,
      explanation: "שכבות מוקדמות לומדות ייצוגים כלליים, שכבות מאוחרות מותאמות למשימה ולכן מעדכנים אותן."
    },
    {
      question: "איזה מהבאים אינו נחשב ליתרון של גישת Transfer Learning?",
      options: [
        "מאפשרת לאמן מודלים מדויקים גם כאשר יש כמות קטנה של דוגמאות מתויגות.",
        "חוסכת בזמן חישוב על ידי שימוש במודל קיים שאומן מראש.",
        "משפרת את ביצועי המודל כאשר תחום המקור שונה מהותית מתחום היעד.",
        "מאפשרת לבצע fine-tuning של שכבות ספציפיות במקום לאמן את כל המודל מחדש."
      ],
      correct: 2,
      explanation: "Transfer Learning פחות יעיל כאשר תחום המקור שונה מהותית מתחום היעד."
    },
    {
      question: "נתונות הטענות הבאות בנוגע לרשתות עמוקות לעומת רשתות שטוחות:\n1. רשתות עמוקות מסוגלות ללמוד ייצוגים היררכיים, כך ששכבות מוקדמות לוכדות תבניות פשוטות והעמוקות לומדות מבנים מורכבים יותר.\n2. ככל שמספר השכבות גדל, כך עולות גם הדרישות לזיכרון ולחישוב, ולעיתים נדרשת אופטימיזציה מיוחדת כדי להאיץ את האימון.\n3. ככל שהרשת עמוקה יותר, כך מובטח שיפור בביצועי המודל, בזכות יכולת ההבעה הגבוהה.\n4. בעיות כמו גרדיאנט מתאפס או מתפוצץ נפוצות יותר ברשתות עמוקות ודורשות לעיתים שימוש בשיטות כמו Batch Normalization או Residual Connections.\n\nבחרו בתשובה הנכונה ביותר:",
      options: [
        "אף טענה אינה נכונה.",
        "רק טענה אחת נכונה.",
        "רק שתי טענות נכונות.",
        "רק שלוש טענות נכונות.",
        "כל ארבע הטענות נכונות."
      ],
      correct: 3,
      explanation: "טענות 1, 2 ו-4 נכונות. טענה 3 שגויה - עומק לא מבטיח שיפור בביצועים."
    },
    {
      question: "אם רשת נוירונים בעלת שכבה חבויה אחת יכולה לקרב כל פונקציה רציפה בדיוק כרצוננו (ע\"פ משפט הקירוב האוניברסלי), מדוע בכל זאת משתמשים ברשתות עמוקות?\n\nבחרו בתשובה הנכונה ביותר:",
      options: [
        "למרות קיומה של רשת כזו תיאורטית, אלגוריתמים של למידה לא תמיד מצליחים למצוא את הפרמטרים הנכונים באימון בפועל.",
        "רשתות שטוחות דורשות לרוב מספר עצום של נוירונים כדי לקרב פונקציות מורכבות.",
        "עומק הרשת מאפשר לה לבנות ייצוגים היררכיים של תכונות, מה שחשוב במיוחד בתחומים כמו ראייה ממוחשבת או עיבוד שפה.",
        "תשובות א׳, ב׳, ו-ג׳ נכונות.",
        "תשובות א׳, ב׳, ו-ג׳ אינן נכונות."
      ],
      correct: 3,
      explanation: "כל שלוש הטענות נכונות - אלו הסיבות לשימוש ברשתות עמוקות."
    },
    {
      question: "נתונה רשת Fully Connected Feedforward הכוללת שתי שכבות: שכבה חבויה אחת עם 10 נוירונים ושכבת פלט אחת לביצוע סיווג בינארי. הקלט לרשת הוא תמונה בגודל 16×16 (כלומר 256 פיקסלים). כמה פרמטרים יש ברשת זו בסך הכל?",
      options: [
        "2,561.",
        "2,570.",
        "2,581.",
        "2,691.",
        "2,571.",
        "תשובות א׳, ב׳, ג׳ ד׳ ו-ה אינן נכונות."
      ],
      correct: 2,
      explanation: "חישוב: (256*10 + 10) + (10*1 + 1) = 2560 + 10 + 10 + 1 = 2581."
    },
    {
      question: "רשת Fully Connected Feedforward עם שכבה אחת בלבד (שכבת הפלט) עושה שימוש בפונקציית אקטיבציה מסוג sigmoid לצורך סיווג בינארי לשתי קטגוריות: \"0\" או \"1\". הפלט של הרשת מפורש כהסתברות שהקלט שייך למחלקה החיובית (\"1\"). המודל מאומן באמצעות עקרון הסבירות המירבית (Maximum Likelihood). איזו מהטענות הבאות מתארת באופן המדוייק ביותר את השפעת תופעת הרוויה (saturation) של פונקציית ה-sigmoid על הגרדיאנט ועל תהליך הלמידה?",
      options: [
        "בעיית הרוויה קיימת רק כאשר המודל חוזה נכון, ולכן אינה פוגעת בתהליך הלמידה.",
        "הגרדיאנט עלול להיות קטן מדי ולחסום את הלמידה בכל פעם שמתרחשת רוויה, בין אם המודל חוזה נכון ובין אם לא.",
        "Cross Entropy מבטלת את השפעת רוויית הסיגמואיד על הגרדיאנט של פונקציית העלות, כאשר המודל חוזה נכון וגם כשלא.",
        "תשובות א׳, ב׳, ו-ג׳ אינן נכונות."
      ],
      correct: 0,
      explanation: "בעיית הרוויה קיימת רק כאשר המודל חוזה נכון (sigmoid קרוב ל-0 או 1), ולכן אינה פוגעת בלמידה כי אין צורך בעדכון."
    },
    {
      question: "נתונה רשת עצבית מסוג Fully Connected Feedforward הכוללת שכבה חבויה אחת, כאשר כל הנוירונים בשכבה זו משתמשים בפונקציית האקטיבציה sigmoid:\nσ(x) = 1/(1+e⁻ˣ).\n\nכעת מחליפים את פונקציות האקטיבציה בשכבה החבויה לפונקציות הטנגנס ההיפרבולי:\ntanh(x) = (eˣ - e⁻ˣ)/(eˣ + e⁻ˣ).\n\nהאם ניתן להתאים את פרמטרי הרשת (משקלים והטיות) כך שהרשת החדשה שבה פונקציות האקטיבציה בשכבה החבויה הן tanh תהיה שקולה, כלומר, תניב את אותה פונקציית מיפוי, לרשת המקורית שבה נעשה שימוש ב-σ? הניחו כי בשתי הרשתות, פונקציית האקטיבציה ביחידת הפלט היא פונקציית הזהות (Identity).",
      options: [
        "ניתן להתאים - לא צפוי שינוי מהותי בפרמטרי הרשת, משום שלשתי הפונקציות תחום ערכים דומה.",
        "ניתן להתאים - נקטין פי שניים את המשקלים וההטיה בכניסה לכל tanh, נקטין פי שניים את המשקל במוצא כל tanh ונתאים את ערך ההטיה בכניסה לשכבת הפלט.",
        "ניתן להתאים - נגדיל פי שניים את המשקלים וההטיה בכניסה לכל tanh, נגדיל פי שניים את המשקל במוצא כל tanh ונתאים את ערך ההטיה בכניסה לשכבת הפלט.",
        "ניתן להתאים - יידרשו משקלים שליליים בלבד, כי tanh מחזירה גם ערכים שליליים, בניגוד ל-sigmoid.",
        "לא ניתן להתאים את פרמטרי הרשת כדי לקבל שקילות.",
        "תשובות א׳, ב׳, ג׳, ד ו-ה אינן נכונות."
      ],
      correct: 1,
      explanation: "מכיוון ש-tanh(x) = 2σ(2x) - 1, צריך להקטין פי 2 את הכניסות ופי 2 את היציאות ולהתאים הטיות."
    },
    {
      question: "שאלות 14,15:\nנתונה פונקציית מחיר (cost function) ריבועית מהצורה:\nf(w) = ½wᵀQw + bᵀw\nכאשר Q ∈ ℝ²ˣ² היא מטריצה סימטרית עם ערכים עצמיים λ₁ = 20, λ₂ = 5 ווקטורים עצמיים מתאימים v₁ᵀ = [1,2]/√5, v₂ᵀ = [-2,1]/√5. הנקודה w*ᵀ = [1,-1] מביאה למינימום את פונקצית המחיר נתונה.\n\nשאלה 14:\nהוקטור b נתון ע״י",
      options: [
        "bᵀ = [-1, 5.5].",
        "bᵀ = [1, -5.5].",
        "bᵀ = [-2, 11].",
        "bᵀ = [2, -11].",
        "bᵀ = [-0.5, 2.75].",
        "תשובות א׳, ב׳, ג׳, ד׳ ו-ה אינן נכונות."
      ],
      correct: 2
    },
    {
      question: "שאלה 15:\nמוסיפים לפונקציית המחיר איבר רגולריזציה מסוג Ridge:\nf(w) = ½wᵀQw + bᵀw + (λ/2)||w||₂²\nעם פרמטר λ = 10. נקודת המינימום w_reg של פונקציית המחיר החדשה עם איבר הרגולריזציה נתונה ע״י:",
      options: [
        "w_regᵀ ≈ [0.27, -0.47].",
        "w_regᵀ ≈ [-0.27, 0.47].",
        "w_regᵀ ≈ [0.47, -0.27].",
        "w_regᵀ ≈ [-0.47, 0.27].",
        "w_regᵀ ≈ [-0.27, 0.27].",
        "w_regᵀ ≈ [0.27, -0.27]."
      ],
      correct: 0
    },
    {
      question: "בעת שימוש בשיטת Gradient Descent לאופטימיזציה של פונקציית עלות קמורה וחלקה, נמצא כי מטריצת ההסיאן (hessian) של הפונקציה בעלת condition number גבוה. איזו מהפעולות הבאות אינה צפויה לשפר את קצב ההתכנסות?",
      options: [
        "ביצוע נרמול (Normalization) או סטנדרטיזציה של משתני הקלט, כך שכל משתנה יהיה בטווח דומה או בעל סטיית תקן אחידה.",
        "שימוש באופטימיזר כמו Adam שמתאים את גודל העדכון בכל כיוון על פי הגרדיאנטים ההיסטוריים של הפונקציה.",
        "שינוי קואורדינטות שמאזן את קנה המידה של הכיוונים השונים.",
        "הוספת Dropout בשכבות החבויות כדי להקטין תלות בין יחידות ולשפר הכללה."
      ],
      correct: 3,
      explanation: "Dropout לא משפר את קצב ההתכנסות - הוא משמש לרגולריזציה ומניעת overfitting."
    },
    {
      question: "בשני מודלים של רשתות נוירונים עמוקות נוסף איבר רגולריזציה שונה לפונקציית העלות:\n• במודל א׳ נעשה שימוש ברגולריזציה מסוג L².\n• במודל ב׳ נעשה שימוש ברגולריזציה מסוג L¹.\n\nאיזו מהטענות הבאות אינה נכונה בנוגע להבדלים בין שני סוגי הרגולריזציה ולהשפעתם על תהליך האופטימיזציה ברשתות עמוקות?",
      options: [
        "רגולריזציה L² נוטה להקטין את ערכי המשקלים אך לרוב תשאיר אותם שונים מאפס.",
        "רגולריזציה L¹ מעודדת דלילות (sparsity), כלומר הרבה משקלים יהיו בדיוק אפס.",
        "שימוש ברגולריזציה L¹ ברשתות עמוקות מצריך אופטימיזציה איטרטיבית, כיוון שאין פתרון אנליטי סגור (closed-form solution).",
        "שימוש ברגולריזציה L² ברשתות עמוקות אינו מצריך אופטימיזציה איטרטיבית, כיוון שיש פתרון אנליטי סגור (closed-form solution)."
      ],
      correct: 3,
      explanation: "ברשתות עמוקות אין פתרון סגור גם עם L² בגלל האי-ליניאריות - תמיד צריך אופטימיזציה איטרטיבית."
    },
    {
      question: "איזו מהתשובות הבאות אינה נכונה לגבי השימוש ב-Dropout במהלך אימון רשתות נוירונים עמוקות?",
      options: [
        "Dropout מונע התחשבות יתר על ידי השמטת אקראית של נוירונים בשכבות במהלך האימון.",
        "Dropout משפר את יכולת ההכללה של הרשת על ידי יצירת \"תת-רשתות\" שונות בכל איטרציה.",
        "Dropout מתאים רק בשכבות החבויות ולא ניתן להפעיל אותו בשכבת הקלט.",
        "בשלב הבדיקה (inference) לא מבצעים Dropout אלא משתמשים בכל הנוירונים עם משקלים מותאמים."
      ],
      correct: 2,
      explanation: "ניתן להפעיל Dropout גם בשכבת הקלט (input dropout)."
    },
    {
      question: "מהם היתרונות של שימוש בשכבות קונבולוציה (Convolutional Layers) לעומת שכבות צפופות (Fully Connected) במשימות חזותיות?\n\nבחרו איזו מהתשובות הבאות אינה נכונה:",
      options: [
        "שכבות קונבולוציה עושות שימוש בהקשר מרחבי.",
        "שכבות קונבולוציה מאפשרות אינוואריאנטיות להזזה.",
        "לשכבות קונבולוציה יש הרבה פחות פרמטרים.",
        "שכבות קונבולוציה דורשות יותר פרמטרים מאשר שכבות צפופות כדי לתפוס את המורכבות של התמונה."
      ],
      correct: 3,
      explanation: "שכבות קונבולוציה דורשות פחות פרמטרים, לא יותר - זה אחד היתרונות שלהן."
    },
    {
      question: "רשת קונבולוציה מקבלת כקלט טנזור קלט ממימד 128x128 העובר דרך שכבת קונבולוציה בעלת 16 פילטרים עם גודל גרעין 3x5, stride 1x3, וללא ריפוד באפסים. מה יהיה גודל טנזור הפלט?\n\nבחרו את התשובה הנכונה:",
      options: [
        "16x126x124.",
        "16x126x42.",
        "16x42x126.",
        "16x125x41.",
        "תשובות א׳, ב׳, ג׳ ו-ד׳ אינן נכונות."
      ],
      correct: 1,
      explanation: "חישוב: (128-3)/1+1=126, (128-5)/3+1=42. הפלט: 16x126x42."
    }
  ]
};
