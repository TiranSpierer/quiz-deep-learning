// Test 004 - מועד ג׳ (חזוי) - מבחן מנובא על סמך חומר הקורס
export default {
  id: "test-004",
  name: "מועד ג׳ (חזוי)",
  description: "20 שאלות חזויות על סמך סיכום הקורס והמבחנים הקודמים",
  questions: [
    {
      question: "בגישת Maximum Likelihood, מהי המטרה העיקרית בבחירת פרמטרי המודל?",
      options: [
        "לבחור את הפרמטרים שממזערים את שגיאת האימון.",
        "לבחור את הפרמטרים שנותנים את ההסתברות הגבוהה ביותר לנתונים שנצפו.",
        "לבחור את הפרמטרים שמקטינים את מספר הפרמטרים ברשת.",
        "לבחור את הפרמטרים שמבטיחים התכנסות למינימום גלובלי.",
        "תשובות א׳, ב׳, ג׳ ו-ד׳ אינן נכונות."
      ],
      correct: 1,
      explanation: "בגישת Maximum Likelihood בוחרים את הפרמטרים שנותנים את ההסתברות הגבוהה ביותר לנתונים שנצפו בפועל."
    },
    {
      question: "נתונות הטענות הבאות בנוגע לפונקציית Softmax:\n1. הפלט הוא וקטור של הסתברויות שסכומן 1.\n2. הפונקציה אינווריאנטית להוספת קבוע לכל איברי הקלט.\n3. כאשר יש הפרשים גדולים בין ערכי הקלט, הפונקציה עלולה להגיע לרוויה.\n4. הגרדיאנט של Softmax תמיד חיובי.\n5. שימוש ב-Negative Log-Likelihood מבטל את בעיית הרוויה של ה-exp.\n\nכמה טענות נכונות?",
      options: [
        "רק טענה אחת נכונה.",
        "רק שתי טענות נכונות.",
        "רק שלוש טענות נכונות.",
        "רק ארבע טענות נכונות.",
        "כל חמש הטענות נכונות."
      ],
      correct: 3,
      explanation: "טענות 1, 2, 3 ו-5 נכונות. טענה 4 שגויה - הגרדיאנט יכול להיות חיובי או שלילי תלוי במחלקה."
    },
    {
      question: "מהו היתרון המרכזי של שימוש בפונקציית מחיר מסוג Negative Log-Likelihood בשילוב עם פונקציית מוצא Sigmoid?",
      options: [
        "הפונקציה הופכת לקמורה (convex) ולכן מובטחת התכנסות למינימום גלובלי.",
        "הפונקציה מבטלת את ה-exp של ה-Sigmoid ובכך מונעת בעיית Vanishing Gradient במוצא.",
        "הפונקציה מאפשרת שימוש בצעד למידה גדול יותר.",
        "הפונקציה מפחיתה את מספר הפרמטרים הנדרשים לאימון.",
        "תשובות א׳, ב׳, ג׳ ו-ד׳ אינן נכונות."
      ],
      correct: 0,
      explanation: "ברגרסיה לוגיסטית, Sigmoid + NLL נותנים פונקציית הפסד קמורה (convex), מה שמבטיח התכנסות למינימום גלובלי. תשובה ב׳ שגויה כי היא מתארת רק שיפור בשכבת הפלט - בעיית Vanishing Gradient עדיין קיימת בשכבות החבויות."
    },
    {
      question: "ברשת נוירונים עם פונקציות הפעלה לינאריות (Identity) בכל השכבות החבויות, איזו מהטענות הבאות נכונה ביותר?",
      options: [
        "הרשת יכולה לייצג כל פונקציה רציפה בדיוק כרצוננו.",
        "הרשת שקולה למודל לינארי ואין משמעות לעומק הרשת.",
        "הרשת תתכנס מהר יותר בזכות פשטות הנגזרות.",
        "הרשת תהיה חסינה לבעיית Vanishing Gradient.",
        "תשובות ב׳ ו-ד׳ נכונות."
      ],
      correct: 1,
      explanation: "עם פונקציות הפעלה לינאריות, הרכבת שכבות לינאריות נותנת טרנספורמציה לינארית אחת - אין משמעות לעומק."
    },
    {
      question: "מהו ההבדל העיקרי בין פונקציות האקטיבציה ReLU ל-Leaky ReLU?",
      options: [
        "Leaky ReLU גזירה בכל נקודה, בעוד ReLU אינה גזירה באף נקודה.",
        "Leaky ReLU מונעת בעיית Dead Neurons על ידי מתן גרדיאנט קטן גם לערכים שליליים.",
        "Leaky ReLU מתאימה רק לבעיות סיווג, בעוד ReLU מתאימה גם לרגרסיה.",
        "Leaky ReLU דורשת יותר חישוב מ-ReLU.",
        "תשובות א׳, ב׳, ג׳ ו-ד׳ אינן נכונות."
      ],
      correct: 1,
      explanation: "Leaky ReLU נותנת גרדיאנט קטן (αx כאשר x<0) גם לערכים שליליים, ובכך מונעת בעיית נוירונים מתים."
    },
    {
      question: "שאלות 6,7:\nנתונה פונקציית מחיר ריבועית מהצורה:\nf(w) = ½wᵀQw + bᵀw\nכאשר Q ∈ ℝ²ˣ² היא מטריצה סימטרית עם ערכים עצמיים λ₁ = 8, λ₂ = 2 ווקטורים עצמיים מתאימים v₁ᵀ = [1,1]/√2, v₂ᵀ = [-1,1]/√2. הוקטור b נתון ע״י bᵀ = [-3, 3].\n\nשאלה 6:\nחשבו את הנקודה w* המביאה למינימום את פונקציית המחיר.",
      options: [
        "w*ᵀ = [0, 0].",
        "w*ᵀ = [0.75, -0.75].",
        "w*ᵀ = [-0.75, 0.75].",
        "w*ᵀ = [1.5, -1.5].",
        "תשובות א׳, ב׳, ג׳ ו-ד׳ אינן נכונות."
      ],
      correct: 1,
      explanation: "נדרש לפתור Qw* = -b. באמצעות פירוק ספקטרלי Q = VΛVᵀ מקבלים w* = -Q⁻¹b."
    },
    {
      question: "שאלה 7:\nמוסיפים לפונקציית המחיר איבר רגולריזציה מסוג Ridge:\nf(w) = ½wᵀQw + bᵀw + (λ/2)||w||₂²\nעם פרמטר λ = 6. נקודת המינימום w_reg של פונקציית המחיר החדשה קרובה ביותר ל:",
      options: [
        "w_regᵀ ≈ [0.32, -0.32].",
        "w_regᵀ ≈ [0.54, -0.54].",
        "w_regᵀ ≈ [-0.32, 0.32].",
        "w_regᵀ ≈ [-0.54, 0.54].",
        "w_regᵀ ≈ [0.75, -0.75]."
      ],
      correct: 0,
      explanation: "עם Ridge, הפתרון הוא w_reg = (Q + λI)⁻¹(-b). הערכים העצמיים החדשים הם 14 ו-8."
    },
    {
      question: "באלגוריתם SGD עם Momentum, מהו התפקיד של פרמטר ה-β (מקדם המומנטום)?",
      options: [
        "קובע את גודל הצעד הבסיסי של האלגוריתם.",
        "קובע כמה משקל לתת לגרדיאנטים מאיטרציות קודמות לעומת הגרדיאנט הנוכחי.",
        "קובע את ההסתברות לדילוג על עדכון מסוים.",
        "קובע את קצב הדעיכה של המשקולות.",
        "תשובות א׳, ב׳, ג׳ ו-ד׳ אינן נכונות."
      ],
      correct: 1,
      explanation: "β קובע את המשקל היחסי בין הכיוון המצטבר מהעבר לבין הגרדיאנט הנוכחי - ככל ש-β גבוה יותר, יש יותר 'זיכרון'."
    },
    {
      question: "מה מייחד את אופטימייזר Adam לעומת SGD עם Momentum?",
      options: [
        "Adam משתמש רק במומנטום של הגרדיאנט, ללא התאמת צעד הלמידה.",
        "Adam מתאים את צעד הלמידה לכל פרמטר בנפרד בהתאם להיסטוריית הגרדיאנטים שלו.",
        "Adam מבטיח התכנסות למינימום גלובלי בכל פונקציה.",
        "Adam דורש פחות זיכרון מ-SGD עם Momentum.",
        "תשובות א׳, ב׳, ג׳ ו-ד׳ אינן נכונות."
      ],
      correct: 1,
      explanation: "Adam משלב מומנטום עם צעד למידה אדפטיבי לכל פרמטר, המבוסס על מומנט ראשון ושני של הגרדיאנטים."
    },
    {
      question: "בעת אימון רשת נוירונים עמוקה, נמצא כי יש פער גדול בין הערכים העצמיים של מטריצת ה-Hessian (condition number גבוה). מה ההשלכה העיקרית של מצב זה?",
      options: [
        "האימון יתכנס מהר יותר בזכות מגוון כיווני הירידה.",
        "צעד למידה קבוע יהיה בעייתי - גדול מדי בכיוונים תלולים וקטן מדי בכיוונים שטוחים.",
        "המודל יגיע בהכרח למינימום גלובלי.",
        "יש להגדיל את מספר השכבות ברשת.",
        "תשובות א׳, ב׳, ג׳ ו-ד׳ אינן נכונות."
      ],
      correct: 1,
      explanation: "condition number גבוה גורם לעקמומיות לא אחידה - צעד למידה קבוע יוביל לזיגזג או התכנסות איטית."
    },
    {
      question: "בגישת Data Augmentation לתמונות, איזו מהפעולות הבאות עלולה להזיק לאימון המודל במשימת זיהוי ספרות כתב יד?",
      options: [
        "שינוי בהירות התמונה.",
        "הוספת רעש קל לתמונה.",
        "סיבוב התמונה ב-180 מעלות.",
        "הזזה קטנה של התמונה.",
        "תשובות א׳, ב׳, ג׳ ו-ד׳ אינן נכונות."
      ],
      correct: 2,
      explanation: "סיבוב ב-180 מעלות יכול להפוך 6 ל-9 ולהפך, ובכך להרוס את התיוג הנכון של הדוגמה."
    },
    {
      question: "ברגולריזציית L² (Weight Decay), מה קורה לנוסחת העדכון של SGD?",
      options: [
        "מתווסף איבר שמכפיל את המשקולות בערך גדול מ-1 בכל איטרציה.",
        "מתווסף איבר שמכווץ את המשקולות לכיוון אפס בכל איטרציה.",
        "מתווסף איבר שמאפס משקולות קטנות באופן אקראי.",
        "אין שינוי בנוסחת העדכון, רק בפונקציית ההפסד.",
        "תשובות א׳, ב׳, ג׳ ו-ד׳ אינן נכונות."
      ],
      correct: 1,
      explanation: "נוסחת העדכון הופכת ל-w ← (1-εα)w - ε∇J, כאשר (1-εα)<1 גורם לדעיכת המשקולות לכיוון אפס."
    },
    {
      question: "רשת קונבולוציה מקבלת כקלט טנזור ממימד 64x64 עם 3 ערוצים (תמונת RGB). הטנזור עובר דרך שכבת קונבולוציה בעלת 32 פילטרים עם גודל גרעין 5x5, stride 2x2, וללא ריפוד באפסים. מה יהיה גודל טנזור הפלט?",
      options: [
        "32x30x30.",
        "32x31x31.",
        "32x29x29.",
        "3x30x30.",
        "תשובות א׳, ב׳, ג׳ ו-ד׳ אינן נכונות."
      ],
      correct: 0,
      explanation: "חישוב: (64-5)/2+1=30. מספר הערוצים בפלט שווה למספר הפילטרים (32). הפלט: 32x30x30."
    },
    {
      question: "בגישת Ensemble, מדוע חשוב שהמודלים יהיו בלתי תלויים (עושים שגיאות שונות)?",
      options: [
        "כי מודלים תלויים דורשים יותר זיכרון.",
        "כי כאשר השגיאות מתואמות (ρ=1), הממוצע לא מפחית את השונות ואין יתרון לשילוב.",
        "כי מודלים תלויים לא ניתנים לאימון במקביל.",
        "כי מודלים תלויים גורמים לבעיית Vanishing Gradient.",
        "תשובות א׳, ב׳, ג׳ ו-ד׳ אינן נכונות."
      ],
      correct: 1,
      explanation: "כאשר ρ=1 (מתאם מלא), השונות נשארת σ² כמו מודל יחיד. ככל ש-ρ קטן יותר, השונות קטנה ל-σ²/M."
    },
    {
      question: "בטכניקת Bagging, כיצד נוצרים סטי האימון השונים?",
      options: [
        "מחלקים את הדאטה ל-M קבוצות שוות ללא חפיפה.",
        "דוגמים מהדאטה המקורי עם החזרה, כך שכל סט בגודל המקורי אך מכיל דוגמאות חוזרות.",
        "מוסיפים רעש שונה לכל עותק של הדאטה.",
        "משתמשים בתת-קבוצות שונות של פיצ'רים לכל מודל.",
        "תשובות א׳, ב׳, ג׳ ו-ד׳ אינן נכונות."
      ],
      correct: 1,
      explanation: "ב-Bagging דוגמים עם החזרה (Bootstrap) - כל דוגמה יכולה להופיע מספר פעמים או לא להופיע כלל."
    },
    {
      question: "בטכניקת Dropout, מה עושים בזמן הטסט (Inference)?",
      options: [
        "ממשיכים להשמיט נוירונים באקראי כמו באימון.",
        "משתמשים בכל הנוירונים, אך מכפילים את המשקולות ב-p (הסתברות ההישארות).",
        "משתמשים רק בנוירונים שהיו פעילים הכי הרבה באימון.",
        "מפעילים Dropout רק בשכבת הפלט.",
        "תשובות א׳, ב׳, ג׳ ו-ד׳ אינן נכונות."
      ],
      correct: 1,
      explanation: "בזמן הטסט משתמשים ברשת המלאה עם משקולות מוכפלות ב-p, כקירוב לממוצע של כל תתי-הרשתות."
    },
    {
      question: "איזו מהטענות הבאות אינה נכונה בנוגע ל-Dropout?",
      options: [
        "Dropout מפחית co-adaptation בין נוירונים.",
        "Dropout מדמה אימון של אנסמבל של תתי-רשתות שונות.",
        "Dropout מקצר את זמן האימון הכולל של הרשת.",
        "Dropout משפר את יכולת ההכללה של המודל."
      ],
      correct: 2,
      explanation: "Dropout לרוב מאריך את זמן האימון פי 2-3, כי כל דוגמה מאומנת על תת-רשת שונה וההתכנסות איטית יותר."
    },
    {
      question: "נתונה רשת Fully Connected Feedforward עם שכבת קלט בגודל 100, שתי שכבות חבויות עם 64 ו-32 נוירונים בהתאמה, ושכבת פלט עם 10 נוירונים. כמה פרמטרים יש ברשת (כולל bias)?",
      options: [
        "8,746.",
        "8,874.",
        "9,066.",
        "9,226.",
        "תשובות א׳, ב׳, ג׳ ו-ד׳ אינן נכונות."
      ],
      correct: 1,
      explanation: "חישוב: (100×64+64) + (64×32+32) + (32×10+10) = 6464 + 2080 + 330 = 8,874."
    },
    {
      question: "בבעיית סיווג רב-מחלקתי עם K מחלקות, מהי ההתאמה האידיאלית בין פונקציית המוצא לפונקציית ההפסד?",
      options: [
        "מוצא: Sigmoid, הפסד: MSE.",
        "מוצא: Softmax, הפסד: Cross-Entropy רב-מחלקתי.",
        "מוצא: ReLU, הפסד: Cross-Entropy.",
        "מוצא: Tanh, הפסד: MSE.",
        "תשובות א׳, ב׳, ג׳ ו-ד׳ נכונות."
      ],
      correct: 1,
      explanation: "Softmax נותן וקטור הסתברויות שסכומו 1, ו-Cross-Entropy רב-מחלקתי מודד את השגיאה בצורה מתאימה."
    },
    {
      question: "מה ההבדל העיקרי בין Early Stopping לבין רגולריזציית L²?",
      options: [
        "Early Stopping עוצר את האימון כשביצועי הוולידציה מתדרדרים, בעוד L² מוסיף קנס על גודל המשקולות.",
        "Early Stopping מתאים רק לרשתות CNN, בעוד L² מתאים לכל סוגי הרשתות.",
        "Early Stopping דורש יותר זיכרון מ-L².",
        "Early Stopping משפר רק את שגיאת האימון, בעוד L² משפר את שגיאת הטסט.",
        "תשובות א׳, ב׳, ג׳ ו-ד׳ אינן נכונות."
      ],
      correct: 0,
      explanation: "Early Stopping היא שיטה מבוססת זמן (מתי לעצור), בעוד L² משנה את פונקציית ההפסד עצמה."
    }
  ]
};
