// Test 005 - מועד ד׳ (חזוי) - מבחן מנובא על סמך חומר הקורס
export default {
  id: "test-005",
  name: "מועד ד׳ (חזוי)",
  description: "20 שאלות חזויות על סמך סיכום הקורס והמבחנים הקודמים",
  questions: [
    {
      question: "בהינתן רשת נוירונים עם פלט y = σ(wᵀx + b) לסיווג בינארי, כאשר σ היא פונקציית Sigmoid ופונקציית ההפסד היא Cross-Entropy. מהו הגרדיאנט של פונקציית ההפסד ביחס ל-w?",
      options: [
        "(σ(wᵀx + b) - y) · x",
        "(y - σ(wᵀx + b)) · x",
        "σ(wᵀx + b) · (1 - σ(wᵀx + b)) · x",
        "log(σ(wᵀx + b)) · x",
        "תשובות א׳, ב׳, ג׳ ו-ד׳ אינן נכונות."
      ],
      correct: 0,
      explanation: "הגרדיאנט של Cross-Entropy עם Sigmoid הוא (ŷ - y)·x = (σ(wᵀx+b) - y)·x, צורה פשוטה ויפה."
    },
    {
      question: "מדוע פונקציית האקטיבציה Softplus (ζ(x) = log(1 + eˣ)) נחשבת לגרסה 'חלקה' של ReLU?",
      options: [
        "כי Softplus מחזירה ערכים רק בתחום [0,1].",
        "כי Softplus גזירה בכל נקודה ומתנהגת כמו x לערכים חיוביים גדולים וכמו 0 לערכים שליליים גדולים.",
        "כי Softplus מהירה יותר לחישוב מ-ReLU.",
        "כי Softplus אינה מגיעה לרוויה באף נקודה.",
        "תשובות א׳, ב׳, ג׳ ו-ד׳ אינן נכונות."
      ],
      correct: 1,
      explanation: "Softplus גזירה בכל נקודה (בניגוד ל-ReLU ב-0) ומתקרבת ל-x לערכים גדולים ול-0 לערכים שליליים."
    },
    {
      question: "נתונות הטענות הבאות בנוגע לשיטת Newton לאופטימיזציה:\n1. השיטה משתמשת במידע מסדר שני (Hessian).\n2. השיטה יכולה להתכנס מהר יותר מ-Gradient Descent כאשר קרובים למינימום.\n3. השיטה מתאימה לרשתות עמוקות עם מיליוני פרמטרים.\n4. השיטה דורשת שה-Hessian תהיה חיובית מוגדרת.\n\nכמה טענות נכונות?",
      options: [
        "רק טענה אחת נכונה.",
        "רק שתי טענות נכונות.",
        "רק שלוש טענות נכונות.",
        "כל ארבע הטענות נכונות.",
        "אף טענה אינה נכונה."
      ],
      correct: 2,
      explanation: "טענות 1, 2 ו-4 נכונות. טענה 3 שגויה - חישוב והיפוך Hessian לא מעשי ברשתות גדולות."
    },
    {
      question: "בעת אימון רשת נוירונים, השגיאה על סט האימון יורדת בהתמדה אך השגיאה על סט הוולידציה מתחילה לעלות. מה הסיבה הסבירה ביותר?",
      options: [
        "קצב הלמידה נמוך מדי.",
        "המודל סובל מ-Underfitting.",
        "המודל סובל מ-Overfitting.",
        "יש בעיה בנתונים של סט הוולידציה.",
        "תשובות א׳, ב׳, ג׳ ו-ד׳ אינן נכונות."
      ],
      correct: 2,
      explanation: "פער גדל בין שגיאת אימון (יורדת) לשגיאת וולידציה (עולה) הוא סימן קלאסי ל-Overfitting."
    },
    {
      question: "ברשת CNN, מהו היתרון המרכזי של שכבת Max Pooling?",
      options: [
        "מגדילה את רזולוציית התמונה.",
        "מוסיפה פרמטרים נלמדים לרשת.",
        "מקטינה את המימד המרחבי ומספקת אינווריאנטיות חלקית להזזות.",
        "משפרת את הגרדיאנטים בשכבות הראשונות.",
        "תשובות א׳, ב׳, ג׳ ו-ד׳ אינן נכונות."
      ],
      correct: 2,
      explanation: "Max Pooling מקטינה מימדים (פחות פרמטרים), ובזכות לקיחת המקסימום מספקת אינווריאנטיות להזזות קטנות."
    },
    {
      question: "שאלות 6,7:\nנתונה פונקציית מחיר ריבועית מהצורה:\nf(w) = ½wᵀQw + bᵀw\nכאשר Q ∈ ℝ²ˣ² היא מטריצה סימטרית עם ערכים עצמיים λ₁ = 16, λ₂ = 4 ווקטורים עצמיים מתאימים v₁ᵀ = [1,0], v₂ᵀ = [0,1]. ידוע כי נקודת המינימום היא w*ᵀ = [0.5, -1].\n\nשאלה 6:\nהוקטור b נתון ע״י:",
      options: [
        "bᵀ = [-8, 4].",
        "bᵀ = [8, -4].",
        "bᵀ = [-4, 2].",
        "bᵀ = [4, -2].",
        "תשובות א׳, ב׳, ג׳ ו-ד׳ אינן נכונות."
      ],
      correct: 0,
      explanation: "מתנאי המינימום: Qw* = -b. מכיוון ש-Q אלכסונית עם 16,4: b = -Qw* = -[16·0.5, 4·(-1)] = [-8, 4]."
    },
    {
      question: "שאלה 7:\nמהו ה-Condition Number של המטריצה Q?",
      options: [
        "2.",
        "4.",
        "8.",
        "16.",
        "20."
      ],
      correct: 1,
      explanation: "Condition Number = λmax/λmin = 16/4 = 4."
    },
    {
      question: "בגישת Transfer Learning, מדוע נהוג להקפיא (freeze) את השכבות הראשונות ולאמן מחדש רק את השכבות האחרונות?",
      options: [
        "כי השכבות הראשונות דורשות יותר זיכרון.",
        "כי השכבות הראשונות לומדות פיצ'רים כלליים (קצוות, טקסטורות) שמתאימים לרוב המשימות, בעוד האחרונות ספציפיות למשימה.",
        "כי השכבות הראשונות לא משפיעות על הביצועים.",
        "כי אי אפשר לחשב גרדיאנט לשכבות הראשונות.",
        "תשובות א׳, ב׳, ג׳ ו-ד׳ אינן נכונות."
      ],
      correct: 1,
      explanation: "שכבות ראשונות לומדות פיצ'רים נמוכי-רמה (קצוות, צבעים) שמתאימים לכל משימה. שכבות עמוקות ספציפיות למשימה."
    },
    {
      question: "מהי הסיבה העיקרית לשימוש ב-Mini-Batch במקום Full-Batch ב-SGD?",
      options: [
        "Mini-Batch מבטיח התכנסות למינימום גלובלי.",
        "Mini-Batch מאפשר עדכונים תכופים יותר, חישוב מהיר יותר, והרעש עוזר לברוח ממינימום מקומי.",
        "Mini-Batch דורש פחות זיכרון אך מוביל תמיד לתוצאות גרועות יותר.",
        "Mini-Batch מתאים רק לבעיות סיווג.",
        "תשובות א׳, ב׳, ג׳ ו-ד׳ אינן נכונות."
      ],
      correct: 1,
      explanation: "Mini-Batch נותן עדכונים תכופים (לא צריך לחכות לכל הדאטה), והרעש בגרדיאנט עוזר לברוח מנקודות אוכף ומינימום מקומי."
    },
    {
      question: "ברשת נוירונים עמוקה עם פונקציות אקטיבציה מסוג Tanh בשכבות החבויות, מה הבעיה העיקרית שעלולה להתרחש?",
      options: [
        "הרשת לא תוכל ללמוד קשרים לא לינאריים.",
        "בעיית Vanishing Gradient - הגרדיאנטים הולכים וקטנים בשכבות המוקדמות.",
        "הרשת תתכנס מהר מדי.",
        "הפלט תמיד יהיה בין 0 ל-1.",
        "תשובות א׳, ב׳, ג׳ ו-ד׳ אינן נכונות."
      ],
      correct: 1,
      explanation: "Tanh מגיעה לרוויה בערכים גדולים/קטנים, הגרדיאנט שלה כמעט אפסי שם, וזה מתרבה בשכבות עמוקות."
    },
    {
      question: "רשת קונבולוציה מקבלת כקלט טנזור ממימד 128x128. הטנזור עובר דרך שכבת קונבולוציה בעלת 64 פילטרים עם גודל גרעין 7x7, stride 1x1, ו-padding של 3 פיקסלים מכל צד. מה יהיה גודל טנזור הפלט?",
      options: [
        "64x128x128.",
        "64x122x122.",
        "64x126x126.",
        "64x124x124.",
        "תשובות א׳, ב׳, ג׳ ו-ד׳ אינן נכונות."
      ],
      correct: 0,
      explanation: "עם padding=3 משני הצדדים, הגודל האפקטיבי הוא 128+6=134. חישוב: (134-7)/1+1=128. הפלט: 64x128x128."
    },
    {
      question: "בנוסחת העדכון של Adam, מהו התפקיד של תיקון ההטיה (bias correction)?",
      options: [
        "למנוע התכנסות למינימום מקומי.",
        "לתקן את העובדה שהמומנטים מאותחלים לאפס ולכן מוטים בתחילת האימון.",
        "להגדיל את צעד הלמידה באיטרציות הראשונות.",
        "למנוע Overfitting.",
        "תשובות א׳, ב׳, ג׳ ו-ד׳ אינן נכונות."
      ],
      correct: 1,
      explanation: "בתחילת האימון, v_t ו-s_t קטנים מדי בגלל אתחול לאפס. תיקון ההטיה (חלוקה ב-1-β^t) מתקן זאת."
    },
    {
      question: "מה ההבדל העיקרי בין שגיאת Bias לשגיאת Variance?",
      options: [
        "Bias נובע ממודל פשוט מדי, Variance נובע ממודל מורכב מדי.",
        "Bias נובע מרעש בדאטה, Variance נובע מבחירת פיצ'רים שגויה.",
        "Bias ו-Variance הם שמות שונים לאותה תופעה.",
        "Bias משפיע רק על סט האימון, Variance משפיע רק על סט הטסט.",
        "תשובות א׳, ב׳, ג׳ ו-ד׳ אינן נכונות."
      ],
      correct: 0,
      explanation: "Bias - המודל לא מצליח ללכוד את הקשר האמיתי (underfitting). Variance - המודל רגיש מדי לנתונים ספציפיים (overfitting)."
    },
    {
      question: "איזו מהשיטות הבאות אינה נחשבת לשיטת רגולריזציה?",
      options: [
        "Dropout.",
        "Early Stopping.",
        "Batch Normalization.",
        "Learning Rate Scheduling.",
        "Data Augmentation."
      ],
      correct: 3,
      explanation: "Learning Rate Scheduling היא טכניקת אופטימיזציה (מתי ואיך לשנות את צעד הלמידה), לא רגולריזציה."
    },
    {
      question: "בעת שימוש ב-Dropout עם p=0.5 (הסתברות השארה), כמה תתי-רשתות שונות קיימות תאורטית ברשת עם 10 נוירונים בשכבות החבויות?",
      options: [
        "10.",
        "20.",
        "100.",
        "1024.",
        "תשובות א׳, ב׳, ג׳ ו-ד׳ אינן נכונות."
      ],
      correct: 3,
      explanation: "כל נוירון יכול להיות פעיל או לא (2 מצבים), עבור 10 נוירונים: 2^10 = 1024 תתי-רשתות אפשריות."
    },
    {
      question: "נתונה רשת Fully Connected Feedforward הכוללת שכבת קלט, שכבה חבויה אחת עם M נוירונים, ושכבת פלט. מוצעת רשת חלופית עם שתי שכבות חבויות עם M/2 נוירונים כל אחת. בהנחה ש-M זוגי ושמספר יחידות הקלט והפלט זהה, האם מספר הפרמטרים:",
      options: [
        "זהה בשתי הרשתות.",
        "גדול יותר ברשת המקורית.",
        "גדול יותר ברשת החלופית.",
        "לא ניתן לקבוע ללא מידע נוסף על גודל הקלט והפלט.",
        "תשובות א׳, ב׳, ג׳ ו-ד׳ אינן נכונות."
      ],
      correct: 3,
      explanation: "התשובה תלויה ביחס בין גודל הקלט/פלט ל-M. צריך מידע נוסף כדי לקבוע."
    },
    {
      question: "מהי הסיבה העיקרית לכך ש-ReLU נמצאה טובה יותר מ-Sigmoid/Tanh בשכבות חבויות של רשתות עמוקות?",
      options: [
        "ReLU מחזירה ערכים בתחום [0,1] מה שמקל על הפרשנות.",
        "ReLU פשוטה לחישוב ולא מגיעה לרוויה בערכים חיוביים, מה שמפחית Vanishing Gradient.",
        "ReLU מבטיחה שכל הנוירונים יהיו פעילים.",
        "ReLU היא פונקציה קמורה.",
        "תשובות א׳, ב׳, ג׳ ו-ד׳ אינן נכונות."
      ],
      correct: 1,
      explanation: "ReLU פשוטה (max(0,x)), הגרדיאנט שלה 0 או 1, אין רוויה לערכים חיוביים - מונע Vanishing Gradient."
    },
    {
      question: "בגישת Negative Log-Likelihood לסיווג רב-מחלקתי עם Softmax, מה קורה כאשר המודל נותן הסתברות גבוהה מאוד למחלקה הנכונה?",
      options: [
        "הגרדיאנט גדול וגורם לעדכון משמעותי.",
        "ה-Loss גבוה מאוד.",
        "ה-Loss קרוב לאפס והגרדיאנט קטן - המודל כמעט לא נענש.",
        "המודל עובר לבעיית Overfitting.",
        "תשובות א׳, ב׳, ג׳ ו-ד׳ אינן נכונות."
      ],
      correct: 2,
      explanation: "כאשר ההסתברות למחלקה הנכונה קרובה ל-1, ה-log(1)≈0, ולכן ה-Loss וגם הגרדיאנט קטנים - זה רצוי!"
    },
    {
      question: "ברגולריזציית L¹ לעומת L², מה ההבדל העיקרי בהשפעה על המשקולות?",
      options: [
        "L¹ מכווצת משקולות לכיוון אפס אך לא מאפסת, L² מאפסת חלק מהמשקולות.",
        "L¹ נוטה לאפס חלק מהמשקולות (sparsity), L² מכווצת את כולן אך לא מאפסת.",
        "אין הבדל מעשי בין השתיים.",
        "L¹ מתאימה רק לרגרסיה, L² מתאימה רק לסיווג.",
        "תשובות א׳, ב׳, ג׳ ו-ד׳ אינן נכונות."
      ],
      correct: 1,
      explanation: "L¹ יוצרת פתרונות sparse (מאפסת משקולות לא חשובות), L² מכווצת את כל המשקולות אך שומרת אותן שונות מאפס."
    },
    {
      question: "מהו Epoch בהקשר של אימון רשת נוירונים?",
      options: [
        "עדכון אחד של הפרמטרים.",
        "מעבר אחד על כל הדוגמאות בסט האימון.",
        "חלוקת הדאטה ל-Mini-Batches.",
        "שלב הבדיקה על סט הטסט.",
        "תשובות א׳, ב׳, ג׳ ו-ד׳ אינן נכונות."
      ],
      correct: 1,
      explanation: "Epoch = מעבר אחד שלם על כל דוגמאות האימון. אם יש 1000 דוגמאות ו-batch size=100, יש 10 iterations ב-epoch."
    }
  ]
};
