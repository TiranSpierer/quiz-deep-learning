// Test 004 - מועד ב׳ - אוגוסט 2025 (מועד 93)
export default {
  id: "test-003",
  name: "מועד ב׳ - אוגוסט 2025",
  description: "20 שאלות מהמבחן הרשמי",
  questions: [
    {
      question: "איזו מהשיטות הבאות נחשבת לשיטת רגולריזציה (regularization)?",
      options: [
        "Dropout.",
        "L² penalty.",
        "Early stopping.",
        "Data Augmentation.",
        "תשובות א׳, ב׳, ג׳ ו-ד׳ נכונות.",
        "תשובות א׳, ב׳, ג׳ ו-ד׳ אינן נכונות."
      ],
      correct: 4,
      explanation: "כל הטכניקות הללו הן שיטות רגולריזציה למניעת overfitting."
    },
    {
      question: "באיזה מצב נשתמש בפונקציית אקטיבציה מסוג sigmoid? בחרו בתשובה הנכונה ביותר.",
      options: [
        "כאשר אנו מבצעים סיווג רב-מחלקתי.",
        "כאשר אנו מבצעים רגרסיה לינארית.",
        "כאשר יש לנו משימה של סיווג בינארי.",
        "כאשר נרצה להחליף את softmax.",
        "תשובות א׳, ב׳, ג׳ ו-ד׳ נכונות.",
        "תשובות א׳, ב׳, ג׳ ו-ד׳ אינן נכונות."
      ],
      correct: 2,
      explanation: "sigmoid מתאימה בעיקר לסיווג בינארי כי היא מחזירה ערך בין 0 ל-1."
    },
    {
      question: "בהנחה שיש ברשותכם רשת מאומנת מראש על מיליוני תמונות של אובייקטים כלליים (כגון ImageNet), ואתם מעוניינים לאמן מודל חדש לזיהוי פתולוגיות רפואיות בצילומי רנטגן, אך במשימה החדשה יש מספר מצומצם של דוגמאות אימון, איזו אסטרטגיית Transfer Learning תיחשב לאפקטיבית ביותר? בחרו את האפשרות המדויקת ביותר.",
      options: [
        "לאמן את כל שכבות הרשת מחדש (training from scratch) כיוון שהמשימה שונה באופן מהותי מהמשימה המקורית ודורשת ייצוגים מותאמים.",
        "לקבע (freeze) את השכבות הראשונות ולאמן מחדש את השכבות העליונות (fine-tuning) כיוון ששכבות מוקדמות לומדות ייצוגים כלליים שניתנים להעברה גם למשימות שונות, וההתאמה נעשית בשכבות הגבוהות.",
        "לקבע (freeze) את כל שכבות הרשת המקורית ולהוסיף שכבה סופית חדשה שתאומן מההתחלה.",
        "להחליף את כל השכבות חוץ מהראשונה, כדי לשמר רק את השלב הראשוני של עיבוד התמונה, המשותף לרוב התחומים."
      ],
      correct: 1,
      explanation: "שכבות מוקדמות לומדות ייצוגים כלליים (קצוות, טקסטורות) שניתנים להעברה, וההתאמה נעשית בשכבות הגבוהות."
    },
    {
      question: "ברשתות קונבולוציה (CNN), נעשה שימוש בשכבות Pooling כחלק מהארכיטקטורה של הרשת. מהו התפקיד המרכזי של שכבות אלו בתהליך הלמידה?\n\nבחרו את האפשרות המדויקת ביותר.",
      options: [
        "מנרמלות את ערכי הקלט כדי לשפר את יציבות האימון.",
        "מוסיפות עומק לרשת ובכך מאפשרות למודל ללמוד ייצוגים מורכבים יותר.",
        "מאיצות את תהליך האופטימיזציה באמצעות שינוי הגרדיאנטים בשכבות הקונבולוציה.",
        "מצמצמות את המימדים המרחביים של הנתונים, מפחיתות את מספר הפרמטרים, ומאפשרות למודל לזהות תכונות באופן חסין לשינויים במקום (translation invariance).",
        "משפרות את דיוק המודל על ידי הקטנת שגיאת הניבוי באופן ישיר."
      ],
      correct: 3,
      explanation: "Pooling מצמצם מימדים מרחביים, מפחית פרמטרים, ומספק translation invariance."
    },
    {
      question: "במהלך יישום של אלגוריתם מורד הגרדיאנט (Gradient Descent) לאופטימיזציה של פונקציית עלות שאינה קמורה, איזו מהטענות הבאות מדויקת ביותר?",
      options: [
        "האלגוריתם מובטח להגיע למינימום הגלובלי, ללא תלות במבנה הפונקציה.",
        "האלגוריתם עלול להתכנס למינימום מקומי או לנקודת אוכף (saddle point), והתוצאה עשויה להיות תלויה בנקודת ההתחלה.",
        "קצב הלמידה (learning rate) משפיע רק על מהירות ההתכנסות, אך אינו משפיע על איכות הפתרון הסופי.",
        "כל עוד מתבצעים עדכונים לפי הגרדיאנט, מובטחת התכנסות לפתרון יציב.",
        "אלגוריתם מורד הגרדיאנט מתאים רק לפונקציות קמורות. עבור פונקציות לא קמורות אין דרך להשתמש בו.",
        "בין התשובות א׳, ב׳, ג׳, ד׳ ו-ה ישנה יותר מתשובה אחת נכונה."
      ],
      correct: 1,
      explanation: "בפונקציות לא קמורות, האלגוריתם עלול להתכנס למינימום מקומי או saddle point, תלוי בנקודת ההתחלה."
    },
    {
      question: "נתונה רשת נוירונים מסוג Fully Connected Feedforward, הכוללת שכבת קלט, שכבה חבויה אחת, ושכבת פלט אחת עם נוירון בודד לצורך סיווג בינארי. בכל שכבה, כל נוירון מקבל קלט מכל הנוירונים בשכבה הקודמת וכולל גם הטיה (bias). מימד הקלט הוא 20x20.\nידוע כי מספר הפרמטרים הכולל (כולל משקלים והטיות) ברשת זו הוא 5,629. מהו מספר הנוירונים בשכבה החבויה?",
      options: [
        "12.",
        "13.",
        "14.",
        "15.",
        "לא ניתן לקבוע מהנתונים.",
        "תשובות א׳, ב׳, ג׳ ד׳ ו-ה אינן נכונות."
      ],
      correct: 2,
      explanation: "חישוב: 400*n + n + n*1 + 1 = 5629 → 401n + n + 1 = 5629 → 402n = 5628 → n = 14."
    },
    {
      question: "במודלים של למידה מפוקחת (supervised learning), מקובל להשתמש ברגולריזציה מסוג L¹ או רגולריזציה מסוג L² כדי להפחית סיכון להתאמת יתר (overfitting). מהו ההבדל המרכזי בין שתי שיטות הרגולריזציה? בחרו את האפשרות המדויקת ביותר.",
      options: [
        "רגולריזציה מסוג L² נוטה לאפס חלק מהמשקלים לחלוטין, ולכן יוצרת פתרונות דלילים (sparse).",
        "רגולריזציה מסוג L¹ שומרת על כל הפרמטרים קטנים אך לא מאפסת אותם.",
        "רגולריזציה מסוג L¹ משמרת את כל הפרמטרים בתוצאה, ולכן אינה מתאימה לצמצום מספר הפיצ׳רים הפעילים במודל.",
        "רגולריזציה מסוג L¹ תמיד עדיפה על רגולריזציה מסוג L², במיוחד ברשתות נוירונים עמוקות.",
        "תשובות א׳, ב׳, ג׳ ו-ד׳ נכונות.",
        "תשובות א׳, ב׳, ג׳ ו-ד׳ אינן נכונות."
      ],
      correct: 5,
      explanation: "כל התשובות שגויות: L¹ (לא L²) יוצרת sparse, L² (לא L¹) שומרת משקלים קטנים אך לא אפס."
    },
    {
      question: "מהי המטרה העיקרית של שימוש בטכניקת Early Stopping במהלך אימון רשתות נוירונים?\n\nבחרו את התשובה הנכונה ביותר.",
      options: [
        "להפסיק את האימון כאשר השיפור בביצועי המודל על קבוצת האימון נעצר או מתחיל להיפגע, כדי לחסוך בזמן חישוב.",
        "להפסיק את האימון כאשר הביצועים על קבוצת הוולידציה מפסיקים להשתפר ומתחילים להידרדר, במטרה למנוע התאמת יתר (overfitting).",
        "להפסיק את האימון לאחר מספר אפוקים קבוע מראש, ללא קשר לביצועי המודל על קבוצות הנתונים.",
        "להפסיק את האימון כאשר שגיאת האימון יורדת מתחת לסף מסוים ומתייצבת.",
        "להפסיק את האימון כשפונקציית ה-Loss משתפרת בהתמדה לאורך מספר אפוקים רצופים.",
        "תשובות א׳, ב׳, ג׳, ד׳ ו-ה אינן נכונות."
      ],
      correct: 1,
      explanation: "Early Stopping עוצר את האימון כשביצועי הוולידציה מפסיקים להשתפר, למניעת overfitting."
    },
    {
      question: "מהו התפקיד המרכזי של פונקציית האקטיבציה (Activation function) ברשת נוירונים?",
      options: [
        "לנרמל או לקודד את נתוני הקלט לפני המעבר לשכבות הבאות.",
        "לאפשר לרשת ללמוד קשרים לא לינאריים בין הקלט לפלט.",
        "לאפשר למודל להתכנס במהירות במהלך תהליך האימון.",
        "להפחית את הסיכון להתאמת יתר (overfitting) באמצעות רגולריזציה פנימית.",
        "תשובות א׳, ב׳, ג׳ ו-ד׳ אינן נכונות."
      ],
      correct: 1,
      explanation: "פונקציות אקטיבציה מכניסות אי-ליניאריות שמאפשרת לרשת ללמוד קשרים מורכבים."
    },
    {
      question: "במהלך אימון רשתות נוירונים עמוקות במיוחד (למשל, בעלות כ-100 שכבות) למשימות סיווג בינארי, נעשה שימוש בפונקציית Sigmoid בשכבת הפלט בשילוב של פונקציות אקטיבציה מסוג tanh ו-ReLU בשכבות הפנימיות. לאחר epoch אחד בלבד, מתברר כי בשכבות מסוימות המשקלים מפסיקים להתעדכן, אף שהרשת לא התכנסה. בדיקה מעמיקה מעלה כי הגרדיאנטים באותן שכבות נעלמים כמעט לגמרי כבר בשלבים מוקדמים של האימון, בעוד שפונקציית ההפסד (Loss) נשארת בטווח סביר.\n\nבהתחשב בכך, אילו מהשינויים האפשריים הבאים עשויים לסייע בצמצום הבעיה?\n1. להגדיל את גודל קבוצת האימון.\n2. להחליף את פונקציות ה-ReLU בפונקציות Leaky ReLU.\n3. להוסיף Batch Normalization לפני כל פונקציית אקטיבציה.\n4. להגדיל את קצב הלמידה (learning rate).\n\nבחרו את האפשרות הנכונה ביותר:",
      options: [
        "רק שינוי אחד מתוך הארבעה עשוי לסייע בצמצום הבעיה.",
        "רק שני שינויים מתוך הארבעה עשויים לסייע בצמצום הבעיה.",
        "שלושה שינויים מתוך הארבעה עשויים לסייע בצמצום הבעיה.",
        "כל ארבעת השינויים המוצעים עשויים לסייע בצמצום הבעיה.",
        "אף אחד מהשינויים המוצעים אינו עשוי לסייע בצמצום הבעיה."
      ],
      correct: 1,
      explanation: "Leaky ReLU ו-Batch Normalization מסייעים בבעיית vanishing gradients. הגדלת dataset או learning rate לא פותרים את הבעיה."
    },
    {
      question: "Dropout היא טכניקת רגולריזציה נפוצה באימון רשתות נוירונים.\n\nבחרו בתשובה המדויקת ביותר.",
      options: [
        "Dropout מוביל לדלילות (sparsity) במשקלים המאומנים של הרשת.",
        "במהלך שלב הבדיקה (test time), Dropout מיושם עם הסתברות הישארות (keep probability) הפוכה.",
        "ככל שהסתברות ההישארות גבוהה יותר, כך האפקט של הרגולריזציה מתחזק.",
        "תשובות א׳ ו-ב׳ נכונות.",
        "תשובות א׳, ב׳ ו-ג׳ נכונות.",
        "תשובות א׳, ב׳, ו-ג׳ אינן נכונות."
      ],
      correct: 5,
      explanation: "Dropout לא מיושם ב-test time, לא מוביל לדלילות במשקלים, והסתברות הישארות גבוהה מחלישה את הרגולריזציה."
    },
    {
      question: "שאלות 12,13:\nנתונה פונקציית עלות (cost function) ריבועית עם איבר רגולריזציה מסוג Ridge מהצורה:\nf(w) = ½wᵀQw + bᵀw + (λ/2)||w||₂²\nכאשר Q ∈ ℝ²ˣ² היא מטריצה סימטרית עם ערכים עצמיים λ₁ = 10, λ₂ = 2.5 ווקטורים עצמיים מתאימים v₁ᵀ = [1,2]/√5, v₂ᵀ = [-2,1]/√5.\nידוע כי עבור ערך רגולריזציה λ = 10, פתרון בעיית המינימום הוא\nw_reg = [7/100, -4/25]ᵀ.\n\nשאלה 12:\nמהו הפתרון w* הממזער את פונקציית העלות כאשר אין איבר רגולריזציה (כלומר, כאשר λ = 0)?",
      options: [
        "w*ᵀ = [1/2, -1/2].",
        "w*ᵀ = [-1/2, 1/2].",
        "w*ᵀ = [1/2, 0].",
        "w*ᵀ = [0, -1/2].",
        "w*ᵀ = [1/2, -1/25].",
        "תשובות א׳, ב׳, ג׳, ד׳ ו-ה אינן נכונות."
      ],
      correct: 0
    },
    {
      question: "שאלה 13:\nהוקטור b נתון ע״י",
      options: [
        "bᵀ = [-0.5, 2.75].",
        "bᵀ = [1, -5.5].",
        "bᵀ = [-1, 5.5].",
        "bᵀ = [0.5, -2.75].",
        "bᵀ = [-0.25, 1.375].",
        "תשובות א׳, ב׳, ג׳, ד׳ ו-ה אינן נכונות."
      ],
      correct: 0
    },
    {
      question: "רשת קונבולוציה מקבלת כקלט טנזור ממימד 256x64 העובר דרך שכבת קונבולוציה בעלת 32 פילטרים עם גודל גרעין 10x6, stride 5x3, וללא ריפוד באפסים. מה יהיה גודל טנזור הפלט?\n\nבחרו את התשובה הנכונה:",
      options: [
        "32x50x20.",
        "32x49x20.",
        "32x50x19.",
        "32x49x19.",
        "32x19x49.",
        "תשובות א׳, ב׳, ג׳, ד׳ ו-ה אינן נכונות."
      ],
      correct: 0,
      explanation: "חישוב: (256-10)/5+1=50, (64-6)/3+1=20. הפלט: 32x50x20."
    },
    {
      question: "בעת אימון רשת נוירונים באמצעות אלגוריתם Stochastic Gradient Descent (SGD), נדרש לבחור גודל מתאים למיני-באצ׳ (mini-batch). מהם היתרונות והחסרונות של שימוש במיני-באצ׳ קטנים לעומת מיני-באצ׳ גדולים?\n\nבחרו את התשובה הנכונה ביותר.",
      options: [
        "מיני-באצ׳ קטנים מובילים לעדכוני משקל תכופים יותר, עם גרדיאנטים רועשים יותר, מה שיכול לסייע ביציאה ממינימום מקומי אך עשוי להאט את ההתכנסות ולדרוש קצב למידה קטן יותר.",
        "מיני-באצ׳ גדולים מפחיתים את רעש הגרדיאנט, מה שתורם להתכנסות יציבה יותר במספר צעדים קטן יותר, אך הם דורשים יותר זיכרון חישובי ועלולים להיתקע במינימום מקומי.",
        "מיני-באצ׳ גדולים צורכים פחות משאבי זיכרון ולכן הם תמיד עדיפים חישובית.",
        "בפועל, גודל המיני-באצ׳ נבחר לעתים קרובות על פי מגבלות זכרון ה-GPU ולא רק משיקולי אופטימיזציה תיאורטיים.",
        "שימוש מיני-באצ׳ קטנים מגביר את הסיכון להתאמת יתר (overfitting) ולכן יש להימנע מהם.",
        "בין התשובות א׳, ב׳, ג׳, ד׳, ו-ה יש יותר מתשובה אחת נכונה."
      ],
      correct: 5,
      explanation: "תשובות א, ב ו-ד נכונות."
    },
    {
      question: "מהו העיקרון המרכזי העומד מאחורי שימוש בטכניקת Dropout במהלך אימון רשתות נוירונים?",
      options: [
        "Dropout מאיץ את זמן האימון על ידי השמטת שכבות שלמות מהרשת בכל אפוק.",
        "Dropout מסייע במניעת Overfitting על ידי הפעלת רשתות משנה שונות בכל איטרציה, ובכך מדמה אנסמבל של מודלים.",
        "Dropout מפחית את עומק הרשת באופן זמני, וכך משפר את קצב ההתכנסות.",
        "Dropout משמש להקטנת ערכי הגרדיאנט בעת Backpropagation לצורך ייצוב האימון."
      ],
      correct: 1,
      explanation: "Dropout מדמה אנסמבל של תת-רשתות שונות בכל איטרציה, מה שמונע overfitting."
    },
    {
      question: "במקרים מסוימים, הוספת רגולריזציה מסוג L² לאימון רשת הובילה להחמרה בביצועים על קבוצת הוולידציה. מה מהבאים הוא הסבר אפשרי לכך?",
      options: [
        "רגולריזציית L² תמיד משפרת את ביצועי המודל על קבוצת הוולידציה, ולכן מדובר כנראה בטעות מדידה.",
        "הרגולריזציה הגבוהה יצרה מודל בעל פרמטרים שמרניים מדי, שפגעו ביכולת המודל לקלוט מבנים מורכבים - תופעה שמזוהה עם Underfitting.",
        "רגולריזציית L² הופכת את פונקציית ה-Loss ללא גזירה, ולכן האימון אינו מתכנס כראוי.",
        "בגלל רגולריזציה מסוג L² המודל מעניש פרמטרים גדולים ולכן מדכא תכונות דומיננטיות בקלט, דבר שפוגע ביכולת ההכללה."
      ],
      correct: 1,
      explanation: "רגולריזציה חזקה מדי גורמת ל-underfitting - המודל לא מצליח ללמוד את המבנים המורכבים בנתונים."
    },
    {
      question: "כיצד משפיעה שיטת ה-Dropout על התלות הדדית (coupling) בין נוירונים באותה שכבה ברשת נוירונים? בחרו את האפשרות המדויקת ביותר.",
      options: [
        "Dropout מגביר את התלות ההדדית (coupling) בין נוירונים, משום שהוא מאלץ אותם ללמוד לעבוד יחד.",
        "Dropout מפחית את התלות ההדדית בין נוירונים, בכך שהוא מאלץ כל נוירון ללמוד תכונות באופן עצמאי ולא להסתמך על נוירונים אחרים.",
        "Dropout אינו משפיע על ה-coupling בין נוירונים, אלא רק על קצב הלמידה וההתכנסות של המודל.",
        "Dropout גורם לדילול (sparsity) בקשרים, אך אינו משנה את אופן שיתוף הפעולה בין הנוירונים.",
        "תשובות א׳, ב׳, ג׳ ו-ד׳ אינן נכונות."
      ],
      correct: 1,
      explanation: "Dropout מפחית co-adaptation בין נוירונים - כל נוירון נאלץ ללמוד תכונות שימושיות באופן עצמאי."
    },
    {
      question: "ברשת Fully Connected Feedforward שבה נעשה שימוש בפונקציית האקטיבציה ReLU בכל השכבות החבויות, אתחול לא תקין של המשקלים עלול לגרום לבעיה חמורה באימון. מהי ההשפעה השכיחה ביותר של בעיה זו? בחרו את האפשרות המדויקת ביותר.",
      options: [
        "הרשת תיכשל בלמידת ייצוגים מורכבים בגלל היעדר אי-ליניאריות.",
        "חלק מהנוירונים יפסיקו להגיב (dead neurons) כבר בתחילת האימון, ולא יתרמו עוד ללמידה.",
        "קצב הלמידה יוגבר מאוד, מה שעלול לגרום לחוסר יציבות.",
        "תשובות א׳, ב׳, ו-ג׳ אינן נכונות."
      ],
      correct: 1,
      explanation: "אתחול לא נכון עם ReLU יכול לגרום ל-dead neurons - נוירונים שהפלט שלהם תמיד 0 ולא מתעדכנים."
    },
    {
      question: "נתונה רשת Fully Connected Feedforward הכוללת מספר שכבות חבויות, כאשר בכל שכבה כל נוירון מחובר לכל נוירון בשכבה הקודמת וכולל גם איבר הטיה (bias). אחת השכבות החבויות כוללת 2M נוירונים. מוצעת רשת חלופית שבה מחליפים את אותה שכבה בשתי שכבות חבויות ברצף, כאשר כל שכבה כוללת M נוירונים, וכל נוירון בהן גם מחובר לכל נוירון בשכבה הקודמת וכולל איבר הטיה.\n\nאיזו מהטענות הבאות נכונה ביותר לגבי מספר הפרמטרים הכולל ברשת המקורית לעומת הרשת החלופית?",
      options: [
        "מספר הפרמטרים ברשת המקורית גדול יותר ממספר הפרמטרים ברשת החלופית.",
        "מספר הפרמטרים ברשת המקורית קטן יותר ממספר הפרמטרים ברשת החלופית.",
        "מספר הפרמטרים ברשת המקורית שווה למספר הפרמטרים הכולל ברשת החלופית.",
        "לא ניתן לקבוע את היחס בין מספר הפרמטרים הכולל בשתי הרשתות (האם גדול, קטן או שווה) מבלי לדעת את מספר השכבות ברשת ואת מספר הנוירונים בכל שכבה.",
        "תשובות א׳, ב׳, ג׳ ו-ד׳ אינן נכונות."
      ],
      correct: 4,
      explanation: "התשובה תלויה במספר הנוירונים בשכבות הסמוכות (A ו-B) לשכבה המוחלפת. ההפרש הוא M(A+B-M): אם A+B>M המקורית גדולה יותר, אם A+B<M החלופית גדולה יותר. תשובה ד׳ שגויה כי היא מציינת שצריך לדעת את מספר השכבות ואת כל הנוירונים - אך למעשה צריך לדעת רק את השכבות הסמוכות."
    }
  ]
};
